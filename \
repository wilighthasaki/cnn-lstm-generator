import tensorflow as tf
import numpy as np
from .get_batches import get_batches

class CNNLSTMGenerator(object):
    '''
    这是用CNN + LSTM + Attention + Decoder组成的图片文字识别模型
    '''
    def __init__(self, height, width, time_steps, num_classes, word2id, embedding_dim=300, hidden_dim=300, is_train=True):

        self.hidden_dim = hidden_dim
        self.word2id = word2id
        self.id2word = {idx: word for word, idx in self.word2id.items()}
        self.num_classes = num_classes
        self.time_steps = time_steps
        self.embedding_dim = embedding_dim

        # 输入输出
        self.image_input = tf.placeholder(tf.float32, [None, height, width, 1])
        self.decode_seqs = tf.placeholder(tf.int64, [None, time_steps])
        self.mask = tf.placeholder(tf.float32, [None, time_steps])
        # default height = 32, width = 128
        self.batch_size = 32

        with tf.variable_scope('embedding_layer'):
            self.embedding_matrix = tf.get_variable('emb_matrix',
                                                    shape=[len(word2id), self.embedding_dim],
                                                    dtype=tf.float32)

            self.decode_seqs_emb = tf.nn.embedding_lookup(self.embedding_matrix,
                                                          self.decode_seqs)
            print(self.embedding_matrix.get_shape().as_list())

        # 网络结构
        with tf.variable_scope('main_model'):
            with tf.variable_scope('conv1'):
                # 卷积层1参数
                self.conv1_weights = tf.get_variable(
                    'conv1_w',
                    shape=[5, 5, 1, 32],
                    dtype=tf.float32
                )
                self.conv1_biases = tf.get_variable(
                    'conv1_b',
                    shape=[32]
                )
                # 卷积层1
                self.conv1 = tf.nn.conv2d(self.image_input,
                                          self.conv1_weights,
                                          strides=[1, 1, 1, 1],
                                          padding='SAME')
                self.relu1 = tf.nn.relu(tf.nn.bias_add(self.conv1, self.conv1_biases))

                # 池化层
                self.pool1 = tf.nn.max_pool(self.relu1,
                                            ksize=[1, 2, 2, 1],
                                            strides=[1, 2, 2, 1],
                                            padding='SAME')

            with tf.variable_scope('conv2'):
                # 卷积层2参数
                self.conv2_weights = tf.get_variable(
                    'conv2_w',
                    shape=[5, 5, 32, 64],
                    dtype=tf.float32
                )
                self.conv2_biases = tf.get_variable(
                    'conv2_b',
                    shape=[64]
                )
                # 卷积层2
                self.conv2 = tf.nn.conv2d(self.pool1,
                                          self.conv2_weights,
                                          strides=[1, 1, 1, 1],
                                          padding='SAME')
                self.relu2 = tf.nn.relu(tf.nn.bias_add(self.conv2, self.conv2_biases))

                # 池化层
                self.pool = tf.nn.max_pool(self.relu2,
                                           ksize=[1, 2, 2, 1],
                                           strides=[1, 2, 2, 1],
                                           padding='SAME')

            with tf.variable_scope('blstm'):
                # reshape
                # input: [batch_size, height / 4, width / 4, 64]
                # output: [batch_size, width / 4, height / 4 * 64]
                self.inputs = tf.transpose(self.pool, perm=[0, 2, 1, 3])
                self.inputs = tf.reshape(self.inputs, [-1, int(width / 4), int(height / 4) * 64])

                # Forward direction cell
                self.lstm_fw_cell = tf.contrib.rnn.LSTMCell(self.hidden_dim, forget_bias=1.0)
                # Backward direction cell
                self.lstm_bw_cell = tf.contrib.rnn.LSTMCell(self.hidden_dim, forget_bias=1.0)

                # Get lstm cell output
                self.outputs, self.final_state = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell,
                                                                    self.lstm_bw_cell,
                                                                    self.inputs,
                                                                    dtype=tf.float32)

                self.final_state = self.final_state[1]
                print(self.final_state.get_shape().as_list())


                self.lstm_outputs = tf.concat((self.outputs[0], self.outputs[1]), axis=2)

            with tf.variable_scope('decoder'):
                self.decode_LSTM_cell = tf.contrib.rnn.LSTMCell(self.hidden_dim)
                # add attention mechanism
                self.attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(512,
                                                                                self.lstm_outputs,
                                                                                memory_sequence_length=[int(width / 4) for _ in range(self.batch_size)])

                self.decode_LSTM_cell = tf.contrib.seq2seq.AttentionWrapper(self.decode_LSTM_cell,
                                                                            self.attention_mechanism,
                                                                            initial_cell_state=self.final_state,
                                                                            attention_layer_size=len(self.word2id))

                self.helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=self.embedding_matrix,
                                                                       start_tokens=tf.tile([word2id['<EOS>']], [self.batch_size]),
                                                                       end_token=word2id['<EOS>']
                                                                       )

                self.decoder = tf.contrib.seq2seq.BasicDecoder(
                    cell=self.decode_LSTM_cell,
                    helper=self.helper,
                    initial_state=self.final_state
                    # initial_state=self.decode_LSTM_cell.zero_state(self.batch_size, tf.float32)
                )
                self.outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(
                    decoder=self.decoder,
                    output_time_major=False,
                    impute_finished=True,
                    maximum_iterations=self.time_steps
                )
            self.softmax_w = tf.get_variable('softmax_w',
                                             [self.num_classes, self.num_classes],
                                             dtype=tf.float32)
            self.softmax_w = tf.reshape(tf.tile(self.softmax_w, [self.batch_size, 1]),
                                        [self.batch_size, self.num_classes, self.num_classes])
            self.softmax_b = tf.get_variable('softmax_b', [self.num_classes], dtype=tf.float32)
            self.logits = tf.matmul(self.outputs[0], self.softmax_w) + self.softmax_b

            self.logits = tf.pad(self.logits,
                                 [[0, 0], [0, self.time_steps - tf.shape(self.logits)[1]], [0, 0]])
            self.y_op = tf.argmax(self.logits, axis=2)
            # 计算损失函数
            self.cost = tf.contrib.seq2seq.sequence_loss(self.logits, self.decode_seqs,
                                                         self.mask,
                                                         average_across_timesteps=True,
                                                         average_across_batch=True)
            self.optimizer = tf.train.AdamOptimizer(0.001)
            self.train_op = self.optimizer.minimize(self.cost)

    def sequences_get_mask(self, sequences, pad_val=0):
        """Return mask for sequences.

        Examples
        ---------
        >>> sentences_ids = [[4, 0, 5, 3, 0, 0],
        ...                  [5, 3, 9, 4, 9, 0]]
        >>> mask = sequences_get_mask(sentences_ids, pad_val=0)
        ... [[1 1 1 1 0 0]
        ...  [1 1 1 1 1 0]]
        """
        mask = np.ones_like(sequences)
        for i, seq in enumerate(sequences):
            for i_w in reversed(range(len(seq))):
                if seq[i_w] == pad_val:
                    mask[i, i_w] = 0
                else:
                    break   # <-- exit the for loop, prepcess next sequence
        return mask

    def train(self, sess, imgs, decode_seqs, num_epochs=10, model_path='outputs/models/model1'):
        saver = tf.train.Saver()
        saver.restore(sess, model_path)

        for epoch in range(num_epochs):
            loss_list = []
            imgs_batches, decode_seqs_batches = get_batches([imgs, decode_seqs], batch_size=32, shuffle=True)
            for batch_idx in range(len(imgs_batches)):
                x = imgs_batches[batch_idx]
                y = decode_seqs_batches[batch_idx]
                mask = self.sequences_get_mask(y, self.word2id['<EOS>'])

                _, cost, y_op = sess.run(
                    [self.train_op,
                     self.cost,
                     self.y_op],
                    feed_dict={self.image_input: x,
                               self.mask: mask,
                               self.decode_seqs: y}
                )
                loss_list.append(cost)
            print('epoch %d finished. Training loss: %.2f' % (epoch, sum(loss_list)/len(loss_list)))
            if epoch % 10 == 0:
                print([[self.id2word[i] for i in j] for j in y_op])
                saver.save(sess, model_path)

    def predict(self, sess, x_input):
        target = sess.run(
            self.y_op,
            feed_dict={
                self.image_input: x_input
            }
        )
        return target
